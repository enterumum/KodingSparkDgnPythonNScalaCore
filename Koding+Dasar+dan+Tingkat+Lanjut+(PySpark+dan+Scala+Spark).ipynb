{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Ref. Big Data Community dan https://spark.apache.org/docs/latest/mllib-naive-bayes.html) MK Analisis Big Data Filkom UB (Imam Cholissodin | imamcs@ub.ac.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latihan Dasar Scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil 1+8 =9\n",
      "Hasil c.tambahdanKali(2,8)._1 = hasil1 != hasil2\n",
      "Hasil c.tambahdanKali(2,8)._2 = 10\n",
      "Hasil c.tambahdanKali(2,8)._3 = 16"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined class Opt\r\n",
       "defined object ScalaDasar\n"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%scala\n",
    "//import java.text.SimpleDateFormat\n",
    "//import java.util.{Calendar, Date}\n",
    "\n",
    "// Membuat class dan fungsi\n",
    "class Opt {\n",
    "  // deklarasi method add secara umum\n",
    "  def add(a: Int, b: Int) = a + b\n",
    "    \n",
    "  // nama method sama, tetapi dengan men-set tipe return-nya\n",
    "  //def add(a: Int, b: Int): Int = a + b\n",
    "    \n",
    "  // men-define method body dalam suatu block dalam kurung kurawal\n",
    "  /*def add(a: Int, b: Int): Int = {\n",
    "    a + b\n",
    "  }*/\n",
    "    \n",
    "  def tambahdanKali(a: Int, b: Int) = {\n",
    "    val hasil1: Int = a + b; // val --> Immutable atau Read only\n",
    "    val hasil2: Int = a * b; // val --> Immutable atau Read only\n",
    "    var Keterangan: String=\"\"; // var --> Mutable atau read-write\n",
    "    if(hasil1 == hasil2){\n",
    "      Keterangan = \"hasil1 = hasil2\";\n",
    "    } else{\n",
    "      Keterangan = \"hasil1 != hasil2\";\n",
    "    }\n",
    "    (Keterangan, hasil1, hasil2) // return langsung 3 nilai\n",
    "  }\n",
    "}\n",
    "\n",
    "object ScalaDasar {\n",
    "   def main(args: Array[String]) {\n",
    "        val c = new Opt()\n",
    "        print(\"Hasil 1+8 =\")\n",
    "        print(c.add(1,8))       \n",
    "        print(\"\\n\")\n",
    "        print(\"Hasil c.tambahdanKali(2,8)._1 = \")\n",
    "        print(c.tambahdanKali(2,8)._1)\n",
    "        print(\"\\n\")\n",
    "        print(\"Hasil c.tambahdanKali(2,8)._2 = \")\n",
    "        print(c.tambahdanKali(2,8)._2)\n",
    "        print(\"\\n\")\n",
    "        print(\"Hasil c.tambahdanKali(2,8)._3 = \")\n",
    "        print(c.tambahdanKali(2,8)._3)\n",
    "       \n",
    "       \n",
    "   }\n",
    "}\n",
    "\n",
    "ScalaDasar.main(Array())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Koding Map Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "def add1(x): \n",
    "    return x+5\n",
    "\n",
    "raw_data = sc.parallelize([1,2,3])\n",
    "rdd = raw_data.map(lambda x: add1(x))\n",
    "#print(rdd.take(3))\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "33.0\n",
      "8.25"
     ]
    },
    {
     "data": {
      "text/plain": [
       "add1: (x: Double)Double\r\n",
       "rdd: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[98] at map at <console>:35\r\n",
       "res62: Array[Double] = Array(6.0, 7.0, 8.0, 12.0)\n"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%scala\n",
    "def add1(x: Double) = x+5\n",
    "\n",
    "//val rdd = sc.parallelize(1 to 3).map(i => add1(i)).count()\n",
    "val rdd = sc.parallelize(List(1, 2, 3, 7)).map(i => add1(i))\n",
    "//val rdd = sc.parallelize(Vector(1, 2, 3)).map(i => add1(i))\n",
    "print(rdd.count()+\"\\n\")\n",
    "print(rdd.sum()+\"\\n\")\n",
    "print(rdd.mean())\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Koding Filter Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 7, 9, 11, 13, 15]\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "def isGanjil(x): \n",
    "    return x%2==1\n",
    "\n",
    "raw_data = sc.parallelize(range(1,16))\n",
    "rdd = raw_data.filter(lambda x: isGanjil(x))\n",
    "#print(rdd.take(3))\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "64.0\n",
      "8.0"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isGanjil: (x: Double)Boolean\r\n",
       "rdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[114] at filter at <console>:34\r\n",
       "res65: Array[Int] = Array(1, 3, 5, 7, 9, 11, 13, 15)\n"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%scala\n",
    "def isGanjil(x: Double) = x%2==1\n",
    "\n",
    "val rdd = sc.parallelize(1 to 15).filter(i => isGanjil(i))\n",
    "//val rdd = sc.parallelize(List(1, 2, 3, 7)).filter(i => isGanjil(i))\n",
    "//val rdd = sc.parallelize(Vector(1, 2, 3)).filter(i => isGanjil(i))\n",
    "print(rdd.count()+\"\\n\")\n",
    "print(rdd.sum()+\"\\n\")\n",
    "print(rdd.mean())\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Koding Reduce Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "def jumlahkan(x,y): \n",
    "    return x+y\n",
    "\n",
    "raw_data = sc.parallelize(range(1,6))\n",
    "#rdd = raw_data.reduce(lambda x,y:x+y)\n",
    "rdd = raw_data.reduce(lambda x,y:jumlahkan(x,y))\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15"
     ]
    },
    {
     "data": {
      "text/plain": [
       "jumlahkan: (x: Int, y: Int)Int\r\n",
       "rdd: Int = 15\n"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%scala\n",
    "def jumlahkan(x: Int, y: Int) = x+y\n",
    "\n",
    "val rdd = sc.parallelize(1 to 5).reduce((i,j) => jumlahkan(i,j))\n",
    "//val rdd = sc.parallelize(List(1, 2, 3, 7)).reduce((i,j) => jumlahkan(i,j))\n",
    "//val rdd = sc.parallelize(Vector(1, 2, 3)).reduce((i,j) => jumlahkan(i,j))\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Koding Pure Lambda Spark (tanpa memberikan nama fungsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "raw_data = sc.parallelize(range(1,6))\n",
    "rdd = raw_data.reduce(lambda x,y:x+y)\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "(lambda x:2*x)(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "raw_data = sc.parallelize([1,2,3])\n",
    "rdd=raw_data.map(lambda x:2*x)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "raw_data = sc.parallelize([(1,2),(3,4),(5,6)])\n",
    "rdd=raw_data.map(lambda x:x[0])\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "raw_data = sc.parallelize([1,2,3])\n",
    "rdd = raw_data.reduce(lambda x,y:x+y)\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "raw_data = sc.parallelize([(1,2),(3,4),(5,6)])\n",
    "rdd=raw_data.map(lambda x:x[0]).reduce(lambda x,y:x+y)\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd: Int = 15\n"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%scala\n",
    "val rdd = sc.parallelize(1 to 5).reduce((i,j) => (i+j))\n",
    "//val rdd = sc.parallelize(List(1, 2, 3, 7)).reduce((i,j) => (i+j))\n",
    "//val rdd = sc.parallelize(Vector(1, 2, 3)).reduce((i,j) => (i+j))\n",
    "val rdd = (i: Int) => { i * 2 }\n",
    "\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_data: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[198] at parallelize at <console>:32\r\n",
       "rdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[199] at map at <console>:33\r\n",
       "res85: Array[Int] = Array(6)\n"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%scala\n",
    "val raw_data = sc.parallelize(List(3))\n",
    "val rdd=raw_data.map(x => 2*x)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_data: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[200] at parallelize at <console>:32\r\n",
       "rdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[201] at map at <console>:33\r\n",
       "res88: Array[Int] = Array(2, 4, 6)\n"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%scala\n",
    "val raw_data = sc.parallelize(List(1,2,3))\n",
    "val rdd=raw_data.map(x => 2*x)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|kolom 0|kolom 1|\n",
      "+-------+-------+\n",
      "|      1| [1, 2]|\n",
      "|      2| [3, 4]|\n",
      "|      3| [5, 6]|\n",
      "+-------+-------+\n",
      "\n",
      "+----------+\n",
      "|kolom 1[0]|\n",
      "+----------+\n",
      "|         1|\n",
      "|         3|\n",
      "|         5|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined object MyScala\n"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%scala\n",
    "object MyScala {\n",
    "    def main(args: Array[String]) {\n",
    "        //import sqlContext.implicits._\n",
    "        val df = Seq((1, Seq(1,2)),(2,Seq(3,4)),(3,Seq(5,6))).toDF(\"kolom 0\",\"kolom 1\")\n",
    "        df.show\n",
    "        \n",
    "        // get data pertama pada kolom 1\n",
    "        val getVal = df.select($\"kolom 1\"(0)).as[Double].show\n",
    "    }\n",
    "}\n",
    "\n",
    "MyScala.main(Array())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6"
     ]
    },
    {
     "data": {
      "text/plain": [
       "raw_data: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[237] at parallelize at <console>:32\r\n",
       "rdd: Int = 6\n"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%scala\n",
    "val raw_data = sc.parallelize(List(1,2,3))\n",
    "val rdd = raw_data.reduce((x,y) => x+y)\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|kolom 0|kolom 1|\n",
      "+-------+-------+\n",
      "|      1| [1, 2]|\n",
      "|      2| [3, 4]|\n",
      "|      3| [5, 6]|\n",
      "+-------+-------+\n",
      "\n",
      "+---------------+\n",
      "|sum(kolom 1[0])|\n",
      "+---------------+\n",
      "|              9|\n",
      "+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined object MyScala\n"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%scala\n",
    "object MyScala {\n",
    "    def main(args: Array[String]) {\n",
    "        //import sqlContext.implicits._\n",
    "        val df = Seq((1, Seq(1,2)),(2,Seq(3,4)),(3,Seq(5,6))).toDF(\"kolom 0\",\"kolom 1\")\n",
    "        df.show\n",
    "        \n",
    "        // get data pertama pada kolom 1\n",
    "        val getVal = df.select($\"kolom 1\"(0)).as[Double].groupBy().sum().show\n",
    "    }\n",
    "}\n",
    "\n",
    "MyScala.main(Array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|item|         other_items|\n",
      "+----+--------------------+\n",
      "| 111|[[111,1.0], [333,...|\n",
      "| 333|                  []|\n",
      "+----+--------------------+\n",
      "\n",
      "+----+-----------+\n",
      "|item|other_items|\n",
      "+----+-----------+\n",
      "| 111|        111|\n",
      "+----+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [item: string, other_items: array<struct<_1:string,_2:double>>]\n"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%scala\n",
    "val df = Seq(\n",
    "  (\"111\", Seq((\"111\", 1.0), (\"333\", 0.5), (\"666\", 0.4))), (\"333\", Seq())\n",
    ").toDF(\"item\", \"other_items\")\n",
    "\n",
    "df.show\n",
    "\n",
    "df.select($\"item\", $\"other_items\"(0)(\"_1\").alias(\"other_items\"))\n",
    "  .na.drop(Seq(\"other_items\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contoh Load data dari File *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitur 1  Fitur 2  Fitur 3  Kelas\n",
      "0  Urgent      Yes      Yes  Party\n",
      "1  Urgent       No      Yes  Study\n",
      "2    Near      Yes      Yes  Party\n",
      "3    None      Yes       No  Party\n",
      "4    None       No      Yes    Pub\n",
      "5    None      Yes       No  Party\n",
      "6    Near       No       No  Study\n",
      "7    Near       No      Yes     TV\n",
      "8    Near      Yes      Yes  Party\n",
      "9  Urgent       No       No  Study\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "#load data dari file csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"data/my/Dataset.csv\");\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.42.217:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spylon-kernel</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=spylon-kernel>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Koding Naive Bayes di Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.classification.{NaiveBayes, NaiveBayesModel}\r\n",
       "import org.apache.spark.mllib.util.MLUtils\r\n",
       "data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[255] at map at MLUtils.scala:84\r\n",
       "training: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[256] at randomSplit at <console>:36\r\n",
       "test: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[257] at randomSplit at <console>:36\r\n",
       "model: org.apache.spark.mllib.classification.NaiveBayesModel = org.apache.spark.mllib.classification.NaiveBayesModel@1b6ee718\r\n",
       "predictionAndLabel: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[272] at map at <console>:40\r\n",
       "accuracy: Double = 0.9814814814814..."
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%scala\n",
    "import org.apache.spark.mllib.classification.{NaiveBayes, NaiveBayesModel}\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "// Load and parse the data file.\n",
    "val data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "// Split data into training (60%) and test (40%).\n",
    "val Array(training, test) = data.randomSplit(Array(0.6, 0.4))\n",
    "\n",
    "val model = NaiveBayes.train(training, lambda = 1.0, modelType = \"multinomial\")\n",
    "\n",
    "val predictionAndLabel = test.map(p => (model.predict(p.features), p.label))\n",
    "val accuracy = 1.0 * predictionAndLabel.filter(x => x._1 == x._2).count() / test.count()\n",
    "\n",
    "// Save and load model\n",
    "model.save(sc, \"target/tmp/myNaiveBayesModel\")\n",
    "val sameModel = NaiveBayesModel.load(sc, \"target/tmp/myNaiveBayesModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "import shutil \n",
    "\n",
    "# Load and parse the data file.\n",
    "data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Split data approximately into training (60%) and test (40%)\n",
    "training, test = data.randomSplit([0.6, 0.4])\n",
    "\n",
    "# Train a naive Bayes model.\n",
    "model = NaiveBayes.train(training, 1.0)\n",
    "\n",
    "# Make prediction and test accuracy.\n",
    "predictionAndLabel = test.map(lambda p: (model.predict(p.features), p.label))\n",
    "accuracy = 1.0 * predictionAndLabel.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
    "print('model accuracy {}'.format(accuracy))\n",
    "\n",
    "# Save and load model\n",
    "output_dir = 'target/tmp/myNaiveBayesModel'\n",
    "shutil.rmtree(output_dir, ignore_errors=True)\n",
    "model.save(sc, output_dir)\n",
    "sameModel = NaiveBayesModel.load(sc, output_dir)\n",
    "predictionAndLabel = test.map(lambda p: (sameModel.predict(p.features), p.label))\n",
    "accuracy = 1.0 * predictionAndLabel.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
    "print('sameModel accuracy {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
